{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Object Tracking Accuracy\n",
    "\n",
    "1. Convert a set of images, which are labeled for YOLO training, into MOTA format\n",
    "2. Use these images as a video for evaluating\n",
    "\n",
    "    Annoyingly, mtometrics and the Yolo-Deepsort code have conflicting dependencies, so a new virtual environment with numpy<2 is necesary. \n",
    "      \n",
    "    **Alternatively**, we can just edit lines 117 and 118 in the `distances.py` module from \n",
    "\n",
    "        objs = np.asfarray(objs)\n",
    "        hyps = np.asfarray(hyps)\n",
    "    \n",
    "    to \n",
    "\n",
    "        objs = np.asarray(objs)\n",
    "        hyps = np.asarray(hyps)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and initializing tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mota_utils import (calculate_iou, centroid_distances, convert_yolo_to_mota, \n",
    "                        display_mota_labels, evaluate_tracker, disp_compare_gt_pred)\n",
    "from detection_helpers import Detector\n",
    "# from tracking_helpers import *\n",
    "from  bridge_wrapper import YoloDeepsortUtility, TRACKER_PARAMS\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Proyectos\\HAU\\codigo\\3-path-planning\\yolov7_deepsort_tracking\\models\\experimental.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "nn_budget not in params_dict. Using default\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(classes = [0,1], conf_thres=TRACKER_PARAMS[\"conf_thres\"], iou_thresh=TRACKER_PARAMS[\"iou_thresh\"]) # class = None means detect all classes. List info at: \"data/coco.yaml\"\n",
    "detector.load_model(TRACKER_PARAMS[\"model_path\"],img_size=1088) # pass the path to the trained weight file\n",
    "tracker = YoloDeepsortUtility(detector=detector,params_dict=TRACKER_PARAMS)\n",
    "# tracker = JustYoloUtility(detector) #Object detection without tracking\n",
    "tracker.satturation = 2 # satturation increase. doesn't seem to affect much\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Track and compare\n",
    "MOTChallenge metrics can be obtained with the `evaluate_tracker` function.\n",
    "\n",
    "Predicted and ground truth bounding boxes can be displayed with `disp_compare_gt_pred` \n",
    "\n",
    "`main_compare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking frame 0/74\n",
      "Tracking frame 1/74\n",
      "Tracking frame 2/74\n",
      "Tracking frame 3/74\n",
      "Tracking frame 4/74\n",
      "Tracking frame 5/74\n",
      "Tracking frame 6/74\n",
      "Tracking frame 7/74\n",
      "Tracking frame 8/74\n",
      "Tracking frame 9/74\n",
      "Tracking frame 10/74\n",
      "Tracking frame 11/74\n",
      "Tracking frame 12/74\n",
      "Tracking frame 13/74\n",
      "Tracking frame 14/74\n",
      "Tracking frame 15/74\n",
      "Tracking frame 16/74\n",
      "Tracking frame 17/74\n",
      "Tracking frame 18/74\n",
      "Tracking frame 19/74\n",
      "Tracking frame 20/74\n",
      "Tracking frame 21/74\n",
      "Tracking frame 22/74\n",
      "Tracking frame 23/74\n",
      "Tracking frame 24/74\n",
      "Tracking frame 25/74\n",
      "Tracking frame 26/74\n",
      "Tracking frame 27/74\n",
      "Tracking frame 28/74\n",
      "Tracking frame 29/74\n",
      "Tracking frame 30/74\n",
      "Tracking frame 31/74\n",
      "Tracking frame 32/74\n",
      "Tracking frame 33/74\n",
      "Tracking frame 34/74\n",
      "Tracking frame 35/74\n",
      "Tracking frame 36/74\n",
      "Tracking frame 37/74\n",
      "Tracking frame 38/74\n",
      "Tracking frame 39/74\n",
      "Tracking frame 40/74\n",
      "Tracking frame 41/74\n",
      "Tracking frame 42/74\n",
      "Tracking frame 43/74\n",
      "Tracking frame 44/74\n",
      "Tracking frame 45/74\n",
      "Tracking frame 46/74\n",
      "Tracking frame 47/74\n",
      "Tracking frame 48/74\n",
      "Tracking frame 49/74\n",
      "Tracking frame 50/74\n",
      "Tracking frame 51/74\n",
      "Tracking frame 52/74\n",
      "Tracking frame 53/74\n",
      "Tracking frame 54/74\n",
      "Tracking frame 55/74\n",
      "Tracking frame 56/74\n",
      "Tracking frame 57/74\n",
      "Tracking frame 58/74\n",
      "Tracking frame 59/74\n",
      "Tracking frame 60/74\n",
      "Tracking frame 61/74\n",
      "Tracking frame 62/74\n",
      "Tracking frame 63/74\n",
      "Tracking frame 64/74\n",
      "Tracking frame 65/74\n",
      "Tracking frame 66/74\n",
      "Tracking frame 67/74\n",
      "Tracking frame 68/74\n",
      "Tracking frame 69/74\n",
      "Tracking frame 70/74\n",
      "Tracking frame 71/74\n",
      "Tracking frame 72/74\n",
      "Tracking frame 73/74\n",
      "Saving tracking results to pred_labels1cust-ds.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf1</th>\n",
       "      <th>idp</th>\n",
       "      <th>idr</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>...</th>\n",
       "      <th>mota</th>\n",
       "      <th>motp</th>\n",
       "      <th>num_transfer</th>\n",
       "      <th>num_ascend</th>\n",
       "      <th>num_migrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>0.828862</td>\n",
       "      <td>0.828096</td>\n",
       "      <td>0.825046</td>\n",
       "      <td>0.915285</td>\n",
       "      <td>0.923792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.823204</td>\n",
       "      <td>0.405187</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf1       idp       idr    recall  precision  ...      mota  \\\n",
       "overall  0.828862  0.828096  0.825046  0.915285   0.923792  ...  0.823204   \n",
       "\n",
       "             motp  num_transfer  num_ascend  num_migrate  \n",
       "overall  0.405187             5           4            4  \n",
       "\n",
       "[1 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_index=\"1b\"\n",
    "tracker.track_video_from_image_folder(img_folder=f\"vid-seg1\", output=f\"pred_labels1cust-ds.txt\")\n",
    "ev = evaluate_tracker(f\"mota_labels{img_index}.txt\", f\"pred_labels1cust-ds.txt\", ignore_id=None, do_print=False, maxiou=0.9) #ignore_id=None does best\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 000030 not found.\n",
      "Frame 000031 not found.\n"
     ]
    }
   ],
   "source": [
    "disp_compare_gt_pred(image_folder=\"vid-seg1\", gt_labels_file=\"mota_labels1b.txt\", pred_labels_file=\"pred_labels1b.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_compare(tracker, img_index, track=True, iou_threshold=10, cent_threshold=0.05, stop_at_frame=None, ignore_id=[0]):\n",
    "    \"\"\" Not recomended, but can perform the whole process of converting YOLO labels to MOTA labels, tracking, and obtaining metrics\"\"\"\n",
    "    \n",
    "    convert_yolo_to_mota(\n",
    "    image_folder=f\"vid-seg{img_index}\",\n",
    "    label_folder=f\"vid-seg{img_index}\",\n",
    "    output_file=f\"mota_labels{img_index}.txt\",\n",
    "    iou_threshold=iou_threshold,\n",
    "    cent_threshold=cent_threshold,\n",
    "    stop_at_frame=stop_at_frame)\n",
    "    \n",
    "    if track:\n",
    "        tracker.track_video_from_image_folder(img_folder=f\"vid-seg{img_index}\", output=f\"pred_labels{img_index}.txt\")\n",
    "\n",
    "    return evaluate_tracker(f\"mota_labels{img_index}.txt\", f\"pred_labels{img_index}.txt\", ignore_id=ignore_id, do_print=False) # ignoring crops (id 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mota</th>\n",
       "      <th>motp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>overall</th>\n",
       "      <td>0.197417</td>\n",
       "      <td>0.329534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             mota      motp\n",
       "overall  0.197417  0.329534"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = main_compare(tracker, track=False, img_index=1, iou_threshold=10, cent_threshold=0.05, ignore_id=[0], stop_at_frame=None)\n",
    "summary\n",
    "#weird. QUalitatively, this looks much worse than cent=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             mota     motp\n",
      "overall  0.326172  0.31707\n"
     ]
    }
   ],
   "source": [
    "\n",
    "convert_yolo_to_mota(\n",
    "    image_folder=\"vid-seg1\",\n",
    "    label_folder=\"vid-seg1\",\n",
    "    output_file=\"mota_labels.txt\",\n",
    "    iou_threshold=10,\n",
    "    cent_threshold=0.05,\n",
    "    stop_at_frame=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_mota_labels(\n",
    "    image_folder=\"vid-seg1\",\n",
    "    mota_labels_file=\"mota_labels1.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_mota_labels(\n",
    "    image_folder=\"vid-seg1\",\n",
    "    mota_labels_file=\"pred_labels1.txt\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
